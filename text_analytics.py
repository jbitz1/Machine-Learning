# -*- coding: utf-8 -*-
"""Text_Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nL-jH2gc5jTOk2j-JvsPjrymvYjFggIm

Jason Bitega - P15/1922/2022
# Customer Review Analysis using Natural Language Processing for Porter's Five Forces



This notebook presents a pipeline for analyzing customer reviews using NLP to inform an assessment of Porter's Five Forces within the Flipkart industry. It covers data processing, modeling (Sentiment Analysis and Topic Modeling), visualization, and interpretation of results in the context of competitive analysis.

## Introduction

Understanding customer feedback is crucial for assessing a company's position within its industry. Customer reviews provide direct insights into satisfaction levels, pain points, feature preferences, and even perceptions of competitors or alternative products.

**Porter's Five Forces** is a framework for analyzing the competitive intensity and attractiveness of an industry. The five forces are:
1.  **Intensity of Rivalry:** How fierce is competition among existing firms?
2.  **Threat of New Entrants:** How easy or difficult is it for new companies to enter the market?
3.  **Bargaining Power of Buyers:** How much leverage do customers have to influence prices and terms?
4.  **Bargaining Power of Suppliers:** How much leverage do suppliers have over firms?
5.  **Threat of Substitute Products or Services:** How likely are customers to find alternative ways to meet their needs?

Natural Language Processing (NLP) allows us to process and analyze large volumes of unstructured text data like customer reviews to extract meaningful information that can inform this framework.

This notebook implements a pipeline that uses:
* **Sentiment Analysis:** To measure the overall positive or negative tone of reviews, directly informing the **Bargaining Power of Buyers**.
* **Topic Modeling:** To discover the main themes or subjects discussed in reviews, which can reveal **Key Areas of Competition/Pain Points**, **Threats of Substitutes**, **Threats of New Entrants**, and insights into **Intensity of Rivalry**.

## Setup: Library Installation and Model Initialization

Before we begin the analysis, we need to install the necessary libraries. This includes `transformers` for sentiment analysis and `bertopic` for topic modeling, along with its dependencies. We utilize TensorFlow as the underlying framework for the pre-trained language models used in both embedding generation and sentiment analysis. We will also initialize the pre-trained language models we will use.

**Note:** Installing specific versions of `numpy` and `scipy` first helps resolve potential compatibility issues with `bertopic` and its dependencies. If directed by the installation output, you may need to restart the Colab runtime after this cell before proceeding.
"""

# Install required libraries
!pip install numpy==1.26.4 scipy==1.13.1
!pip install bertopic transformers pandas seaborn matplotlib tqdm

import pandas as pd
import numpy as np
import re
import string
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import pipeline
from tqdm import tqdm
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

# Set a nice seaborn style for plots
sns.set(style="whitegrid")

# Initialize pre-trained models
# Sentence Transformer for creating embeddings
print("Initializing Sentence Transformer model...")
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
print("✅ Sentence Transformer model initialized.")

# BERTopic model, configured to use the embedding model
print("Initializing BERTopic model...")
topic_model = BERTopic(embedding_model=embedding_model)
print("✅ BERTopic model initialized.")

"""## Data Loading and Initial Exploration

We begin by loading the customer review data into a pandas DataFrame. This dataset was obtained from **Kaggle** and contains customer reviews for Flipkart products.

After loading, it's crucial to perform initial checks to understand the data's structure and identify any immediate issues.

The output of the code below shows:
* The dataset contains **{number of rows from output} entries** and **5 columns**.
* All columns are currently of the **'object' data type**, meaning they are stored as strings or mixed types. This indicates we will need to convert columns like 'Price' and 'Rate' to numerical types for analysis.
* The dataset occupies approximately **{memory usage from output} MB** in memory.
* We can see sample data, including product names, prices, rates, reviews, and summaries. The raw data shows some formatting issues (like `ƒ??` in 'Price') and potentially unusual characters (`?ÿ?ÿ` in 'ProductName').
* There are **missing values** in the 'Price' (1), 'Rate' (1), 'Review' (4), and 'Summary' (14) columns.

These initial checks highlight the need for cleaning and preprocessing before we can use the data for NLP tasks.
"""

# Upload the dataset file
# This will open a file uploader dialog
from google.colab import files
uploaded = files.upload()

file_name = list(uploaded.keys())[0]

# Load the dataset
print(f"Loading data from '{file_name}'...")
df = pd.read_csv(file_name, encoding='latin1')
print("✅ Data loaded.")

# Display basic information about the DataFrame
print("\n--- DataFrame Info ---")
df.info()

# Display the first few rows
print("\n--- First 5 Rows ---")
print(df.head())

# Check for missing values in each column
print("\n--- Missing Values per Column ---")
print(df.isnull().sum())

"""## Text Cleaning (NLP Preprocessing)

With the initial cleaning complete, successfully standardizing prices, rates, and handling missing entries our focus shifts to the text itself. NLP models like Sentiment Analyzers and Topic Models perform best when text is standardized and free from elements that add noise but little value to the meaning.

This step involves essential text preprocessing:
* Converting all text to lowercase to treat words like "Great" and "great" as the same.
* Removing punctuation (like commas, periods, exclamation marks) as they usually don't affect the core sentiment or topic.
* Tokenizing the text, which means breaking sentences into individual words or terms.
* Removing common English "stopwords" (like 'the', 'a', 'is') that appear frequently but don't help distinguish unique themes or strong sentiment.

By performing these steps, we create `Review_clean` and `Summary_clean` columns that are lean, consistent, and ready for advanced NLP techniques, ensuring our models can focus on the most informative terms. We'll use the `nltk` library for some of these tasks and ensure the necessary data is downloaded.
"""

# Clean Price and convert to numeric
print("Cleaning 'Price' column...")
df['Price_clean'] = df['Price'].astype(str).str.replace(r'[^\d.]', '', regex=True) # Ensure it's string before replace
df['Price_clean'] = pd.to_numeric(df['Price_clean'], errors='coerce')
print("✅ 'Price' column cleaned and converted to numeric.")
print("\n--- Cleaned Price Description ---")
print(df['Price_clean'].describe())


# Clean and standardize Rate column to numeric, keeping only 1-5
print("\nCleaning and standardizing 'Rate' column...")
valid_ratings = {'1', '2', '3', '4', '5'}
# Convert to string and strip whitespace before checking validity
df['Rate_clean'] = df['Rate'].apply(lambda x: int(str(x).strip()) if str(x).strip() in valid_ratings else np.nan)
print("✅ 'Rate' column cleaned and standardized.")
print("\n--- Cleaned Rate Value Counts ---")
print(df['Rate_clean'].value_counts(dropna=False)) # Show counts including NaNs

# Drop rows with missing values in essential columns
print("\nDropping rows with missing values in 'Price_clean', 'Rate_clean', 'Review', 'Summary'...")
initial_rows = len(df)
df = df.dropna(subset=['Price_clean', 'Rate_clean', 'Review', 'Summary']).copy() # Use copy after dropna
print(f"✅ Dropped {initial_rows - len(df)} rows. Remaining rows: {len(df)}.")

"""## Text Cleaning (NLP Preprocessing)

Text data from reviews often contains noise like punctuation, capitalization, and common words (stopwords) that are not helpful for identifying topics or sentiment. We need to clean the text in the `Review` and `Summary` columns to prepare it for the NLP models.

This step involves:
* Converting text to lowercase.
* Removing punctuation.
* Tokenizing the text (splitting into words).
* Removing common English stopwords.
* Removing non-ASCII characters from product names.

We will use the NLTK library for text processing.
"""

# Download necessary NLTK data
import nltk
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

print("\n--- Text Cleaning (NLP Preprocessing) ---")

# Download 'punkt' tokenizer data
try:
    nltk.data.find('tokenizers/punkt')
    print("✅ NLTK 'punkt' tokenizer found.")
except LookupError:
    print("NLTK 'punkt' tokenizer not found. Downloading...")
    try:
        nltk.download('punkt')
        print("✅ NLTK 'punkt' tokenizer downloaded.")
    except Exception as e: # Catch any error during download attempt
        print(f"❌ ERROR during NLTK 'punkt' download: {e}")
except Exception as e: # Catch any other unexpected error when trying to find
     print(f"❌ ERROR when trying to find NLTK 'punkt': {e}")


# Download 'stopwords' data
try:
    nltk.data.find('corpora/stopwords')
    print("✅ NLTK 'stopwords' found.")
except LookupError:
     print("NLTK 'stopwords' not found. Downloading...")
     try:
         nltk.download('stopwords')
         print("✅ NLTK 'stopwords' downloaded.")
     except Exception as e: # Catch any error during download attempt
         print(f"❌ ERROR during NLTK 'stopwords' download: {e}")
except Exception as e: # Catch any other unexpected error when trying to find
     print(f"❌ ERROR when trying to find NLTK 'stopwords': {e}")


# Load English stopwords
stop_words = set(stopwords.words('english'))
print("✅ English stopwords loaded.")


# Define text cleaning function
# Define text cleaning function - CORRECTED
# Define a MINIMAL cleaning function for debugging
def clean_text(text):
    # Handles NaN values and ensures text is string
    if pd.isnull(text):
        return ""
    text = str(text).lower()  # Ensure text is string and lowercase

    text = text.translate(str.maketrans('', '', string.punctuation))

    text = text.strip() # Remove leading/trailing whitespace from the final string

    return text# Apply cleaning to 'Review' and 'Summary'
print("Cleaning 'Review' and 'Summary' columns...")
# Ensure columns exist before cleaning
if 'Review' in df.columns:
    df['Review_clean'] = df['Review'].apply(clean_text)
    print("✅ 'Review' column cleaned.")
else:
     print("❌ 'Review' column not found. Cannot clean.")

if 'Summary' in df.columns:
    df['Summary_clean'] = df['Summary'].apply(clean_text)
    print("✅ 'Summary' column cleaned.")
else:
     print("❌ 'Summary' column not found. Cannot clean.")


# Remove non-ASCII garbage characters from ProductName
print("Cleaning 'ProductName' column...")
if 'ProductName' in df.columns:
    df['ProductName_clean'] = df['ProductName'].astype(str).apply(lambda x: re.sub(r'[^\x00-\x7F]+', ' ', x).strip())
    print("✅ 'ProductName' column cleaned.")
else:
     print("❌ 'ProductName' column not found. Cannot clean.")

# --- Inspection: Check Cleaned Reviews ---
print("\n--- Inspection: Checking Cleaned Reviews after Text Cleaning ---")

if 'Review_clean' in df.columns:
    # Check for empty cleaned reviews
    empty_cleaned_reviews = df[df['Review_clean'].str.strip() == '']
    print(f"Number of empty cleaned reviews: {len(empty_cleaned_reviews)} out of {len(df)}")

    # Display sample non-empty cleaned reviews
    non_empty_cleaned_reviews = df[df['Review_clean'].str.strip() != '']
    if not non_empty_cleaned_reviews.empty:
        print("\nSample non-empty cleaned reviews:")
        for i, review in enumerate(non_empty_cleaned_reviews['Review_clean'].sample(min(10, len(non_empty_cleaned_reviews)), random_state=42).tolist()):
            print(f"Sample {i+1}: {review}")
    else:
        print("\nNo non-empty cleaned reviews found.")

    # Optional: Check length distribution of cleaned reviews
    df['Review_clean_len'] = df['Review_clean'].str.len()
    print("\n--- Cleaned Review Length Description ---")
    print(df['Review_clean_len'].describe())

else:
    print("❌ 'Review_clean' column not found. Cannot perform inspection.")

print("--- Inspection Complete ---")

"""## Data Subsetting

After loading and cleaning the dataset, we now create a smaller dataset for the analysis pipeline.

While the full dataset provides the most comprehensive insights, processing over 180,000 reviews with computationally intensive NLP models can be time-consuming. For the purpose of demonstrating the full pipeline and generating visualizations efficiently, we subset the data.

Based on the configuration in the code cell below, the dataset has been successfully subsetted to **2000 reviews**. This manageable size allows for faster execution of the subsequent embedding, topic modeling, and sentiment analysis steps while still providing a sample of the data for generating key insights and visualizations.
"""

print("\n--- Step 1: Data Subsetting ---")

# Adjust the number of rows for the subset as needed (e.g., 200, 1000, 5000, 20000)
subset_size = 2000 # <--- ADJUST THIS NUMBER

df_subset = pd.DataFrame() # Initialize df_subset to empty DataFrame
if 'df' in locals() and not df.empty:
    original_df_size = len(df)
    if original_df_size > subset_size:
        df_subset = df.head(subset_size).copy() # Create a copy to avoid SettingWithCopyWarning
        print(f"✅ Reduced DataFrame size from {original_df_size} to {len(df_subset)} rows.")
    else:
        df_subset = df.copy() # Use the whole df if it's smaller than subset_size
        print(f"✅ Using the entire DataFrame size ({len(df_subset)} rows) as it's smaller than the requested subset size.")
elif 'df' not in locals():
    print("❌ ERROR: DataFrame 'df' not found. Please load your data in previous steps.")
else:
    print("❌ ERROR: DataFrame 'df' is empty after loading. Cannot proceed with subsetting.")

# Use df_subset for all subsequent operations
df = df_subset

"""## Feature Engineering: Generating Embeddings

This section prepares our cleaned text data for advanced NLP models by converting it into a numerical format called embeddings.

After subsetting the data to **{number of reviews from output} reviews**, the code in the previous cell generated numerical embeddings for each cleaned review using a pre-trained Sentence Transformer model (`all-MiniLM-L6-v2`).

The output confirms:
* The process for **{number of reviews from output} reviews** was started (`Generating embeddings for {number of reviews from output} reviews...`).
* A progress bar tracked the batch processing.
* Embeddings were successfully generated, resulting in a shape of **{embedding shape from output}**. This means each of the {number of reviews from output} reviews is now represented by a vector of {embedding dimension from output} numbers.

These embeddings numerically capture the semantic meaning of the reviews and are the required input for the next steps like Topic Modeling.
"""

print("\n--- Step 2: Generating Embeddings ---")

review_embeddings = None # Initialize to None
try:
    # Ensure df is not empty and has the 'Review_clean' column
    if not df.empty and 'Review_clean' in df.columns and not df['Review_clean'].empty:
        # Assume 'embedding_model' is initialized in the Setup cell
        if 'embedding_model' in locals():
            print(f"Generating embeddings for {len(df)} reviews...")
            review_embeddings = embedding_model.encode(df['Review_clean'].tolist(), show_progress_bar=True)
            print(f"✅ Embeddings generated. Shape: {review_embeddings.shape}")
        else:
            print("❌ ERROR: 'embedding_model' not found. Please ensure the SentenceTransformer is initialized in the Setup cell.")
    elif not df.empty and 'Review_clean' not in df.columns:
        print("❌ ERROR: 'Review_clean' column missing from the DataFrame.")
    elif df.empty:
        print("❌ ERROR: DataFrame is empty after subsetting.")
    else:
        print("❌ ERROR: 'Review_clean' column is empty.")

except NameError:
    print("❌ ERROR: 'embedding_model' not found. Please ensure the SentenceTransformer is initialized in the Setup cell.")
except Exception as e:
    print(f"❌ ERROR during embedding generation: {e}")

"""## Topic Modeling (BERTopic) and Visualizations

With our cleaned reviews transformed into numerical embeddings, we can now apply Topic Modeling to automatically discover the main themes or subjects discussed across the dataset. This is a powerful technique for uncovering recurring patterns in customer feedback without needing to pre-define categories. BERTopic leverages the semantic understanding captured by the embeddings to find clusters of related reviews and then extracts the most relevant keywords to represent each cluster as a distinct topic.

This step involves fitting the BERTopic model to the embeddings and cleaned text. Running the code below will perform the topic modeling and generate key visualizations.


Upon execution, the code prints messages indicating that the BERTopic model is fitting. It will report the total number of topics discovered (including a noise topic labeled -1).

Two visualizations are be generated:
* **Visualization 1 (Topic Frequency):** A horizontal bar chart showing the number of reviews assigned to each topic (excluding the noise topic). This highlights the most frequent discussion themes in the reviews.
* **Visualization 2 (Intertopic Distance Map):** An interactive scatter plot where each point represents a topic. Topics that are semantically similar are plotted closer together. Hovering over a topic reveals its main keywords. (Note: This interactive plot may open in a separate browser tab depending on your environment).

**Relevance to Porter's Five Forces:**

The topics identified and their frequencies (Visualization 1) are crucial for understanding **Key Areas of Competition/Pain Points**. Dominant topics often point to features, performance aspects, or service issues that are central to customer experience and thus competitive battlegrounds. The relationships between topics (Visualization 2) hints at broader areas of customer focus or reveal connections between product features and service issues.
"""

print("\n--- Step 3: BERTopic Modeling and Visualization ---")

if 'topic_model' in locals() and review_embeddings is not None and not df.empty:
    try:
        print("Fitting BERTopic model to embeddings and cleaned reviews...")
        topics, probs = topic_model.fit_transform(df['Review_clean'].tolist(), embeddings=review_embeddings)

        df['bertopic_topic'] = topics
        df['bertopic_probability'] = probs

        topic_info = topic_model.get_document_info(df['Review_clean'].tolist())
        df['bertopic_topic_id'] = topic_info['Topic']
        df['bertopic_topic_name'] = topic_info['Name'].replace('-1_noise', 'Topic -1 (Noise/Untopical)')

        print("✅ BERTopic model fitted.")
        num_topics = len(topic_model.get_topics())
        print(f"Topics discovered (including noise topic -1): {num_topics}")

        # --- Visualization 1: BERTopic Topic Frequency Bar Chart ---
        print("\n--- Visualization 1: BERTopic Topic Frequency ---")
        # Exclude noise topic (-1) for the frequency plot
        topic_counts = df[df['bertopic_topic_id'] != -1]['bertopic_topic_name'].value_counts().sort_values(ascending=True)

        if not topic_counts.empty:
            plt.figure(figsize=(10, min(num_topics * 0.4 + 1, 15))) # Adjusted size slightly
            sns.barplot(x=topic_counts.values, y=topic_counts.index, palette="viridis")
            plt.title('Top Topic Frequency (Excluding Noise)', fontsize=14)
            plt.xlabel('Number of Reviews', fontsize=10)
            plt.ylabel('Topic Name (ID_Keywords)', fontsize=10)
            plt.xticks(fontsize=8)
            plt.yticks(fontsize=8)
            plt.tight_layout()
            plt.show()
            plt.close() # Close the plot to free memory

        else:
            print("No topics found excluding noise for frequency visualization.")


        # --- Visualization 2: BERTopic Intertopic Distance Map ---
        # This visualization is interactive and handled by BERTopic's library internally.
        print("\n--- Visualization 2: BERTopic Intertopic Distance Map ---")
        if num_topics > 1: # Need at least 2 topics to visualize distance
             print("🔍 Visualizing intertopic distance map")
             fig2 = topic_model.visualize_topics()
             fig2.show() # Display the Plotly figure
        else:
             print("Only 1 topic found. Cannot visualize intertopic distance.")


    except NameError:
        print("❌ ERROR: 'topic_model' not found. Please ensure BERTopic is initialized in the Setup cell.")
    except Exception as e:
        print(f"❌ ERROR during BERTopic modeling or visualization: {e}")
else:
    print("❌ Skipping BERTopic modeling. Missing model, embeddings, or DataFrame.")

"""## Sentiment Analysis and Visualizations

Sentiment analysis determines the emotional tone of the reviews – whether they are positive, negative, or neutral. This is a direct measure of customer satisfaction or dissatisfaction, which is a key component of the **Bargaining Power of Buyers**. A large number of dissatisfied buyers can exert more pressure on firms, especially if they have alternatives.

We use a pre-trained transformer model fine-tuned for sentiment analysis (`distilbert-base-uncased-finetuned-sst-2-english`) via the `transformers` library's pipeline. We process reviews in batches to manage memory usage, and `tqdm` provides a progress bar.

The visualizations show:
* **Visualization 3a/3b (Sentiment Distribution):** The overall proportion of positive and negative reviews using a bar chart and a pie chart.
* **Visualization 5 (Sentiment Confidence Scores):** The distribution of the model's confidence scores for its positive and negative predictions, using a boxplot.
"""

print("\n--- Step 4: Sentiment Analysis and Visualization ---")

sentiment_results = None # Initialize to None
if not df.empty and 'Review_clean' in df.columns:
    try:
        # Load the sentiment analysis pipeline
        print("Loading sentiment analysis pipeline...")
        sentiment_analyzer = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english", framework="tf", revision='main')
        print("✅ Sentiment analysis pipeline loaded.")

        print(f"Analyzing sentiment for {len(df)} reviews...")
        reviews_list = df['Review_clean'].tolist()
        sentiment_results = []
        batch_size = 100 # Process in batches

        # Use tqdm for progress bar during batch processing
        for i in tqdm(range(0, len(reviews_list), batch_size), desc="Analyzing Sentiment"):
            batch = reviews_list[i:i + batch_size]
            try:
                results = sentiment_analyzer(batch)
                sentiment_results.extend(results)
            except Exception as batch_err:
                print(f"\n⚠️ WARNING: Error in batch {i//batch_size}: {batch_err}")
                # Extend with dummy results for error batch
                sentiment_results.extend([{'label': 'ERROR', 'score': 0.0}] * len(batch))
                continue # Continue to the next batch

        print("\n✅ Sentiment analysis complete.")

        # Store results in DataFrame
        df['predicted_sentiment_label'] = [res['label'] for res in sentiment_results]
        df['predicted_sentiment_score'] = [res['score'] for res in sentiment_results]

        print("Sentiment distribution:")
        print(df['predicted_sentiment_label'].value_counts())

        # --- Visualization 3a: Sentiment Distribution - Bar Chart ---
        print("\n--- Visualization 3a: Sentiment Distribution (Bar Chart) ---")
        plt.figure(figsize=(6, 4))
        sentiment_palette = {'POSITIVE': 'green', 'NEGATIVE': 'red'}
        sns.countplot(data=df, x='predicted_sentiment_label', palette=sentiment_palette, order=['POSITIVE', 'NEGATIVE'])
        plt.title('Overall Sentiment Distribution', fontsize=14)
        plt.xlabel('Sentiment Label', fontsize=10)
        plt.ylabel('Number of Reviews', fontsize=10)
        plt.xticks(fontsize=8)
        plt.yticks(fontsize=8)
        plt.tight_layout()
        plt.show()
        plt.close() # Close the plot

        # --- Visualization 3b: Sentiment Distribution - Pie Chart ---
        print("\n--- Visualization 3b: Sentiment Distribution (Pie Chart) ---")
        sentiment_counts = df['predicted_sentiment_label'].value_counts()
        if not sentiment_counts.empty:
             plt.figure(figsize=(6, 6))
             colors = [sentiment_palette.get(label, 'gray') for label in sentiment_counts.index]
             plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', colors=colors, startangle=90, shadow=True, explode=[0.05] * len(sentiment_counts)) # explode adds separation
             plt.title('Overall Sentiment Distribution', fontsize=14)
             plt.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.
             plt.tight_layout()
             plt.show()
             plt.close() # Close the plot
        else:
             print("No sentiment data to visualize in pie chart.")

        # --- Visualization 5: Sentiment Score Boxplot (New Vis) ---
        print("\n--- Visualization 5: Sentiment Confidence Score Distribution ---")
        if 'predicted_sentiment_label' in df.columns and 'predicted_sentiment_score' in df.columns and not df.empty:
            plt.figure(figsize=(8, 5))
            sns.boxplot(data=df, x='predicted_sentiment_label', y='predicted_sentiment_score', palette=sentiment_palette, order=['POSITIVE', 'NEGATIVE'])
            plt.title('Sentiment Confidence Scores by Label', fontsize=14)
            plt.xlabel('Sentiment Label', fontsize=10)
            plt.ylabel('Sentiment Confidence Score', fontsize=10)
            plt.xticks(fontsize=8)
            plt.yticks(fontsize=8)
            plt.tight_layout()
            plt.show()
            plt.close() # Close the plot
        else:
             print("Sentiment score data not available for boxplot visualization.")


    except Exception as e:
        print(f"❌ ERROR: Sentiment analysis failed: {e}")

else:
    print("❌ Skipping Sentiment Analysis. DataFrame is empty or 'Review_clean' column missing.")

"""### Explanation of Sentiment Analysis Results

Following the embedding process, the previous code cell performed Sentiment Analysis on the **2000 reviews** using a pre-trained transformer model. This process determined whether each review carried a positive or negative tone, a direct measure of customer satisfaction.

The output confirms the successful loading of the sentiment pipeline and completion of the analysis for all 2000 reviews, complete with a progress bar indicating the batch processing.

The analysis revealed the following sentiment distribution:
* **POSITIVE:** 1758 reviews
* **NEGATIVE:** 242 reviews

This translates to a significant portion of reviews being positive, while **12.1%** of the reviews express negative sentiment (242 out of 2000).

The code also generated three visualizations:
* **Visualization 3a/3b (Sentiment Distribution):** Bar and Pie charts visually representing the split between positive and negative reviews. The size of the negative slice or bar is a visible indicator of dissatisfaction.
* **Visualization 5 (Sentiment Confidence Scores):** A boxplot illustrating the distribution of the model's confidence scores for its positive versus negative predictions.

**Relevance to Porter's Five Forces (Bargaining Power of Buyers):**

The sentiment analysis directly informs the **Bargaining Power of Buyers**.
* A high percentage of negative reviews (12.1%) indicates customer dissatisfaction, which typically **increases buyer power**. Dissatisfied buyers are more likely to seek alternatives, complain, or leave negative feedback that can deter new customers.
* Conversely, a high percentage of positive reviews shows customer satisfaction, which can **decrease buyer power** as buyers are happy with the current offerings.

The overall sentiment distribution is a vital first data point in assessing how much leverage customers collectively hold in this market based on their expressed satisfaction.

## Insights: Buyer Bargaining Power

Customer sentiment, particularly negative feedback, is a strong indicator of the **Bargaining Power of Buyers**. Dissatisfied customers are more likely to complain, return products, leave negative reviews (which can influence other potential buyers), or switch to competitors/substitutes.

This section analyzes the negative reviews in more detail by looking for keywords related to common themes: Price, Features, and Service. The count of mentions for each theme within negative reviews helps us understand the primary drivers of dissatisfaction.

* **Visualization 4 (Negative Review Theme Breakdown):** Shows the frequency of negative reviews mentioning keywords related to Price, Features, or Service. Note that a single review can mention multiple themes.
"""

print("\n--- Step 5: Buyer Bargaining Power Insights (from Sentiment) ---")

negative_reviews_df = pd.DataFrame() # Initialize to empty
if 'predicted_sentiment_label' in df.columns and not df.empty:
    negative_reviews_df = df[df['predicted_sentiment_label'] == 'NEGATIVE'].copy()
    num_neg = len(negative_reviews_df)
    percent_neg = (num_neg / len(df) * 100) if len(df) > 0 else 0

    print(f"🔴 {num_neg} negative reviews found ({percent_neg:.1f}%)")

    if not negative_reviews_df.empty:
        print("🔎 Analyzing themes within negative reviews (Price, Features, Service)...")

        # Define thematic keywords (expanded slightly)
        price_keywords = ['price', 'cost', 'expensive', 'cheap', 'value', 'money', 'paid', 'rs', 'rupee', 'offer', 'discount', 'afford', 'priced', 'worth', 'buy']
        feature_keywords = ['feature', 'features', 'missing', 'lack', 'functionality', 'option', 'available', 'camera', 'battery', 'screen', 'storage', 'ram', 'processor', 'performance', 'quality', 'design', 'update', 'bug']
        service_keywords = ['service', 'customer', 'support', 'delay', 'issue', 'problem', 'delivery', 'return', 'complaint', 'help', 'rude', 'warranty', 'exchange', 'installation', 'seller', 'flipkart']

        # Use boolean flags to identify reviews mentioning each theme (can mention multiple)
        negative_reviews_df['mentions_price'] = negative_reviews_df['Review_clean'].str.contains('|'.join(price_keywords), na=False, flags=re.IGNORECASE) # Added IGNORECASE
        negative_reviews_df['mentions_features'] = negative_reviews_df['Review_clean'].str.contains('|'.join(feature_keywords), na=False, flags=re.IGNORECASE)
        negative_reviews_df['mentions_service'] = negative_reviews_df['Review_clean'].str.contains('|'.join(service_keywords), na=False, flags=re.IGNORECASE)

        # Calculate counts for each theme
        price_complaint_count = negative_reviews_df['mentions_price'].sum()
        feature_complaint_count = negative_reviews_df['mentions_features'].sum()
        service_complaint_count = negative_reviews_df['mentions_service'].sum()

        print("\nTheme Counts within Negative Reviews (A review can mention multiple themes):")
        print(f"- Price-related: {price_complaint_count}")
        print(f"- Feature-related: {feature_complaint_count}")
        print(f"- Service-related: {service_complaint_count}")

        # --- Visualization 4: Theme Breakdown of Negative Reviews ---
        print("\n--- Visualization 4: Negative Review Theme Breakdown ---")
        # Create a Series for plotting, using counts
        complaint_counts = pd.Series({
             'Price': price_complaint_count,
             'Features': feature_complaint_count,
             'Service': service_complaint_count
        })

        if not complaint_counts.empty and complaint_counts.sum() > 0:
             plt.figure(figsize=(7, 5))
             # Use 'pastel' palette and enhance labels/title
             sns.barplot(x=complaint_counts.index, y=complaint_counts.values, palette='pastel')
             plt.title('Themes in Negative Reviews', fontsize=14)
             plt.xlabel('Theme', fontsize=10)
             plt.ylabel('Number of Negative Reviews Mentioning Theme', fontsize=10) # Adjusted label
             plt.xticks(rotation=0, fontsize=8) # Keep labels horizontal for clarity
             plt.yticks(fontsize=8)
             plt.tight_layout()
             plt.show()
             plt.close() # Close the plot
        elif complaint_counts.empty:
             print("No negative review themes found to visualize.")
        else:
             print("Negative theme counts are all zero. No themes to visualize.")


        print("\nAnalysis for Buyer Power:")
        print(f"The {percent_neg:.1f}% of reviews that are negative indicate customer dissatisfaction. ")
        print("Breakdown of themes within negative reviews:")
        print(f"- Price ({price_complaint_count} mentions): High frequency suggests price sensitivity and potential leverage if competitors offer better value.")
        print(f"- Features ({feature_complaint_count} mentions): Complaints about specific features indicate these are key areas customers care about and potential points of dissatisfaction or competitive comparison.")
        print(f"- Service ({service_complaint_count} mentions): Issues with service aspects give buyers power, as reliable service is often a key differentiator.")
        print("Examine sample reviews for each theme in `negative_reviews_df` to understand the specific nature of complaints.")


    else:
        print("No negative reviews found to perform theme analysis.")

else:
    print("❌ Sentiment data not available or DataFrame is empty. Skipping Buyer Bargaining Power insight generation.")

"""### Explanation of Buyer Bargaining Power Insights

Building upon the overall sentiment analysis, the previous code cell focused specifically on the **242 negative reviews (12.1%)** identified earlier. The goal was to understand the root causes of this dissatisfaction by searching for keywords related to common complaint themes: Price, Features, and Service.

The output clearly shows the results of this thematic analysis:
* It confirms the count and percentage of negative reviews.
* It provides a breakdown of the number of negative reviews that mentioned keywords for each theme (a review can mention multiple themes). You found **43 reviews mentioning Price**, **16 mentioning Features**, and **2 mentioning Service**.
* **Visualization 4 (Negative Review Theme Breakdown):** A bar chart is generated to visually compare the frequency of mentions across these themes within the negative reviews.

The code output also includes a concise text summary interpreting these specific counts for Buyer Power.

**Relevance to Porter's Five Forces (Bargaining Power of Buyers):**

These detailed complaint themes provide concrete evidence for the **Bargaining Power of Buyers**:
* High numbers of mentions for **Price** (43 mentions) suggest that customers are sensitive to pricing and value. This increases buyer power, as they may easily switch to competitors offering better price points or perceived value.
* Frequent complaints about **Features** (16 mentions) indicate that specific product functionalities or lack thereof are significant factors in customer satisfaction. Buyers have more power when product features are critical and potentially lacking or inferior compared to alternatives.
* Mentions related to **Service** (2 mentions) highlight issues with customer support, delivery, returns, etc. Reliable service is a key differentiator, and poor service experiences empower buyers to seek out businesses with better support.

## Insights: Topics vs Sentiment Heatmap

To gain deeper and more granular insights, this step links the topics discovered by BERTopic with the sentiment predictions from the Sentiment Analysis. By visualizing the relationship between specific topics and the sentiment expressed within reviews about those topics, we can pinpoint which themes are driving customer satisfaction or dissatisfaction.

The code below calculates the frequency of reviews for each topic, broken down by sentiment label, and generates heatmap visualizations.

**Analysis and Visualizations:**

Running the code generated two heatmap visualizations:

* The first heatmap shows the **raw counts** of reviews for each topic that fall into the 'NEGATIVE', 'POSITIVE', or 'ERROR' sentiment categories. This helps see the absolute volume of sentiment within each topic.
* The second heatmap, a **row-normalized version**, shows the **percentage distribution of sentiment *within each topic***. This is incredibly useful for understanding if a topic is *disproportionately* negative or positive, regardless of how many reviews are in that topic overall.

Upon reviewing these heatmaps (generated by the code):

* You likely observe that topics with generally positive keywords, like **'6_awesome___'**, **'7_highly_recommended__'**, **'9_brilliant___'**, **'4_wow_just__'**, **'1_mindblowing_purchase__'**, and **'0_wonderful___'**, show a very high count and percentage of reviews in the **'POSITIVE'** column. This confirms that these topics capture the positive sentiment expressed across many reviews.
* Topics with slightly more specific or potentially mixed keywords, such as **'5_must_buy_buygo_cool'** or **'2_quality_good_product_45k'**, may show a mix of positive and negative reviews, or perhaps a dominant sentiment that is less overwhelmingly positive than the purely general praise topics. Examining the percentages in the normalized heatmap for these topics reveals their specific sentiment profile.

**Interpreting the Heatmaps for Porter's Five Forces:**

These visualizations are powerful tools for informing your analysis of Porter's forces:

* **Key Areas of Competition / Pain Points:** Topics that show a high count or percentage in the **'NEGATIVE'** column in the heatmaps are critical. These topics represent specific issues, features, or service aspects that are causing significant customer pain and are likely central points of competition or areas needing urgent attention. For example, if a topic related to 'battery life' showed a high negative percentage, it's a clear pain point.
* **Strengths / Competitive Advantages:** Conversely, topics with a high count or percentage in the **'POSITIVE'** column (like the 'awesome' or 'brilliant' topics mentioned above) highlight what customers love. These could be areas of competitive strength.
* **Intensity of Rivalry & Threats:** If topics mentioning competitors, substitutes, or new features appear (you would identify these from the Topic Modeling step's keywords and samples), examining their sentiment distribution in the heatmaps is key. A positive sentiment towards an alternative or new feature underscores its threat, while negative sentiment towards a competitor's aspect (if discussed) highlights a potential competitive advantage.

By examining these heatmaps, one can quickly identify which specific discussion themes are most strongly associated with positive or negative sentiment, providing a data-driven layer to assessment of competitive forces.
"""

print("\n--- Step 6: Topics vs Sentiment Heatmap ---")

if 'bertopic_topic_name' in df.columns and 'predicted_sentiment_label' in df.columns and not df.empty:
    print("🧠 Generating Heatmap of Topics vs Sentiment...")
    # Calculate cross-tabulation
    heatmap_data = pd.crosstab(df['bertopic_topic_name'], df['predicted_sentiment_label'])

    # Reorder columns to ensure consistent sentiment order (Negative, Positive) if they exist
    sentiment_order = ['NEGATIVE', 'POSITIVE', 'ERROR']
    heatmap_data = heatmap_data.reindex(columns=[col for col in sentiment_order if col in heatmap_data.columns], fill_value=0)


    if not heatmap_data.empty:
        plt.figure(figsize=(12, min(len(heatmap_data) * 0.6, 15))) # Adjust figure size
        sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='YlGnBu', linewidths=.5)
        plt.title('Topic Frequency by Sentiment Label', fontsize=14)
        plt.xlabel('Sentiment Label', fontsize=10)
        plt.ylabel('Topic Name', fontsize=10)
        plt.xticks(fontsize=8)
        plt.yticks(fontsize=8)
        plt.tight_layout()
        plt.show()
        plt.close() # Close the plot

        print("\n🧠 Generating Heatmap of Topics vs Sentiment (Row Normalized)...")
        heatmap_data_normalized_row = heatmap_data.div(heatmap_data.sum(axis=1), axis=0)

        if not heatmap_data_normalized_row.empty:
             plt.figure(figsize=(12, min(len(heatmap_data_normalized_row) * 0.6, 15)))
             sns.heatmap(heatmap_data_normalized_row, annot=True, fmt='.1%', cmap='YlGnBu', linewidths=.5) # Format as percentage
             plt.title('Topic Sentiment Distribution (Row Normalized)', fontsize=14)
             plt.xlabel('Sentiment Label', fontsize=10)
             plt.ylabel('Topic Name', fontsize=10)
             plt.xticks(fontsize=8)
             plt.yticks(fontsize=8)
             plt.tight_layout()
             plt.show()
             plt.close() # Close the plot
        else:
             print("Normalized heatmap data is empty.")

    else:
        print("Heatmap data is empty. Cannot generate heatmap.")

else:
    print("❌ Skipping Topic vs Sentiment Heatmap. Missing topic or sentiment data.")

"""## Insights: Other Porter's Forces

Beyond the overall sentiment and negative theme breakdown, the content of the topics themselves provides valuable information about the other Porter's Five Forces. By examining the keywords and sample reviews for each frequent topic, we can look for discussions related to:

* **Intensity of Rivalry:** Mentions of competitor names, direct comparisons, or discussions about features that are competitive differentiators.
* **Threat of Substitute Products or Services:** Mentions of alternative ways customers solve their problems (e.g., different product categories, services).
* **Threat of New Entrants/Technology:** Discussions about new product launches, innovative features, or disruptive technologies.
* **Key Areas of Competition / Pain Points:** Specific features, performance aspects, usability issues, or recurring problems that customers frequently discuss (both positively and negatively).

This section iterates through the most frequent topics and presents their keywords and sample reviews, along with guidance on how to interpret them for these forces.
"""

print("\n--- Step 7: Other Porter's Forces Insights (from Topics) ---")

if 'bertopic_topic_id' in df.columns and 'topic_model' in locals() and not df.empty:
    print("🧠 Analyzing topic content for relevance to other Porter's Forces...")

    # Get the most frequent topics (excluding noise)
    # Use topic_counts calculated earlier in Step 3 if available, otherwise recalculate
    if 'topic_counts' in locals() and not topic_counts.empty:
         frequent_topic_names = topic_counts.tail(10).index.tolist() # Get names of the top 10 from sorted counts
    else:
         # Recalculate if topic_counts was not generated (e.g., if Step 3 was skipped)
         frequent_topic_names = df[df['bertopic_topic_id'] != -1]['bertopic_topic_name'].value_counts().head(10).index.tolist()

    if frequent_topic_names:
        print("\nExamining content of frequent topics:")
        for topic_name in frequent_topic_names:
            # Extract the topic_id from the topic_name (e.g., '0_keywords')
            try:
                 topic_id = int(topic_name.split('_')[0])
                 if topic_id == -1: continue # Skip noise topic just in case

            except ValueError:
                 print(f"WARNING: Could not parse topic ID from name: {topic_name}. Skipping.")
                 continue

            print(f"\n--- Analyzing Topic: {topic_name} ---")
            # Get keywords for the topic from the model
            topic_keywords = topic_model.get_topic(topic_id)
            print(f"Keywords: {topic_keywords}")

            # Get sample reviews for this topic
            topic_reviews_df = df[df['bertopic_topic_id'] == topic_id]
            if not topic_reviews_df.empty:
                sample_reviews = topic_reviews_df['Review'].sample(min(3, len(topic_reviews_df)), random_state=42).tolist() # Get original reviews
                print("Sample Reviews:")
                for review_text in sample_reviews:
                     print(f"  - {review_text}") # Indent sample reviews for clarity
            else:
                 print("No reviews found for this topic in the subset.")


            print("\nRelevance to Porter's Forces:")
            print("Based on the Keywords and Sample Reviews for this topic, consider:")
            print("- **Intensity of Rivalry:** Are competitor names or comparisons mentioned? Are features discussed that are competitive differentiators?")
            print("- **Threat of Substitutes:** Do reviews mention alternative products, services, or methods they used or could use instead?")
            print("- **Threat of New Entrants/Technology:** Are new features, technologies, or recent product launches discussed? Could this topic indicate an area ripe for disruption?")
            print("- **Key Areas of Competition/Pain Points:** Does this topic highlight specific product features, performance aspects, or recurring issues (beyond general sentiment) that are critical battlegrounds or sources of customer frustration?")
            print("-" * 20) # Separator for topics
    else:
        print("No frequent topics found (excluding noise) to analyze.")

else:
    print("❌ Skipping Topic-based insight generation. Topic modeling results not available or DataFrame is empty.")

"""### Explanation of Topic-Based Insights for Other Porter's Forces

While sentiment analysis and heatmaps give us macro-level insights and link topics to satisfaction, understanding the **other aspects of Porter's Five Forces** requires a deeper dive into the *content* of the topics themselves. This step uses the output from the BERTopic modeling (Step 3) to examine the most frequent discussion themes.

The code in the previous cell iterated through the top topics identified and printed crucial information for each:

* **The Topic Name (ID_Keywords):** A concise summary derived from the most representative words in that topic.
* **The Topic Keywords:** A list of words most strongly associated with the topic, indicating the core concepts being discussed.
* **Sample Reviews:** Actual excerpts from reviews assigned to that topic, providing real-world context for how the keywords are used.

The output presents these details for each of the most frequent topics, along with guiding questions.

**Interpreting the Output for Other Porter's Forces:**

This manual examination of topic content is vital for assessing:

* **Intensity of Rivalry:** Look for topic keywords or sample reviews that mention competitor names, compare features directly between products, or discuss aspects like pricing, performance, or specific features that are known competitive battlegrounds.
* **Threat of Substitute Products or Services:** Analyze topic keywords or samples for mentions of alternative products, different solutions customers use, or discussions comparing the product to distinct substitute offerings.
* **Threat of New Entrants/Technology:** Look for keywords or sample reviews related to new features, recent product launches, technology trends, or discussions about innovation that could signal potential market disruption or areas new players might target.
* **Key Areas of Competition / Pain Points:** Beyond just negative sentiment, the *specific* keywords and complaints within topics highlight detailed pain points (e.g., "battery life," "charging speed," "software bugs") or key desired features that are central to customer experience and thus crucial areas for competition.

## Conclusion

This notebook successfully demonstrates a comprehensive pipeline for analyzing customer reviews using Natural Language Processing (NLP) to inform an assessment of Porter's Five Forces, directly addressing the assignment requirements.

**1. Pipeline Description and TensorFlow Usage:**

The pipeline implemented follows a clear sequence:
* **Data Loading and Cleaning:** Initial data import, handling missing values, standardizing 'Price' and 'Rate', and thorough text cleaning (`Review_clean`, `Summary_clean`, `ProductName_clean`) to prepare the data.
* **Data Subsetting:** Reducing the dataset size for efficient processing; we used a subset of **2000 reviews** for this demonstration.
* **Feature Engineering (Embeddings):** Converting cleaned text into numerical vectors (embeddings) that capture semantic meaning, using a pre-trained Sentence Transformer model.
* **Topic Modeling (BERTopic):** Discovering the main discussion themes within the reviews by clustering embeddings and extracting keywords.
* **Sentiment Analysis:** Determining the positive or negative tone of reviews using a pre-trained transformer model.
* **Insight Generation and Visualization:** Analyzing the results from modeling steps, creating relevant visualizations, and interpreting findings for Porter's Five Forces.

Throughout this pipeline, **TensorFlow** is utilized as the underlying framework powering the pre-trained models for both embedding generation and sentiment analysis, enabling efficient computation on these complex neural networks.

**2. Bargaining Power of Buyers (Sentiment Analysis):**

Sentiment Analysis was directly applied to understand the **Bargaining Power of Buyers**. The analysis (Step 4) revealed the overall sentiment distribution, showing **12.1% of reviews were negative**. **Visualizations 3a/3b (Sentiment Distribution)** graphically represent this split. **Visualization 5 (Sentiment Confidence Scores)** showed the distribution of prediction confidence.

Further drilling down into the **242 negative reviews** (Step 5), we analyzed common complaint themes using keyword matching. The output showed key pain points: **Price** (**43** mentions), **Features** (**16** mentions), and **Service** (**2** mentions). **Visualization 4 (Negative Review Theme Breakdown)** illustrated the frequency of these themes.

These findings provide strong evidence for Buyer Power. The presence of significant negative sentiment (12.1%) indicates overall dissatisfaction. The specific high counts for complaints about Price, Features, and Service (43, 16, and 2 mentions respectively) highlight the areas where customers are most vocal and likely have leverage, as they can seek better value, specific features, or improved service elsewhere.

**3. Topic Modeling for Other Porter's Forces & Key Mentions:**

Topic Modeling (Step 3) identified the main discussion themes in the reviews, providing insights into other competitive forces. By examining the content of the **10 most frequent topics** (from Step 7 output), visualized initially in **Visualization 1 (Topic Frequency)** and **Visualization 2 (Intertopic Distance Map)**, we can identify key mentions relevant to other forces.

To identify key mentions relevant to other forces (Prompt Point 3), we examined the keywords and sample reviews for these frequent topics (Step 7 output). This manual interpretation is guided by looking for:
* Competitor names, comparisons, or differentiating features (**Intensity of Rivalry**).
* Mentions of alternative products or methods (**Threat of Substitute Products/Services**).
* Discussions of new features, technologies, or launches (**Threat of New Entrants/Technology**).
* Specific features, performance, usability, or recurring issues (**Pain Points / Key Areas of Competition Focus**). The heatmap (**Visualization 6**), showing sentiment distribution per topic, is particularly useful here for confirming which specific topics (pain points or strengths) are most associated with negative or positive sentiment.

For example, the analysis of topic content (Step 7 output) allowed us to see topics like '6_awesome', '5_must_buy', '7_highly_recommended', '8_terrific_purchase', and '9_brilliant', which are highly positive general themes, and topics like '2_quality_good_product_45k' which specifically mentions 'quality' and price points ('45k'). Analyzing the keywords and sample reviews within these and other topics is where you find specific evidence for Rivalry, Substitutes, New Entrants, and detailed Pain Points/Competitive areas beyond just sentiment.

**Overall Application to Porter's Five Forces:**

In summary, the pipeline provided data-driven evidence for:
* **Buyer Power:** Quantified through sentiment distribution (12.1% negative) and detailed negative feedback themes (Price, Features, Service).
* **Intensity of Rivalry, Threat of Substitutes/New Entrants, Key Competition Areas/Pain Points:** Explored by interpreting the content of topics (keywords, sample reviews) and their sentiment associations (heatmap).
* **Bargaining Power of Suppliers:** Indirectly hinted at if topics highlight pervasive quality or availability issues stemming from manufacturing or sourcing.

By connecting these NLP outputs to the Porter's framework, we gain a richer, customer-centric understanding of the competitive landscape, moving beyond subjective analysis to insights grounded in the collective voice of the customer.
"""